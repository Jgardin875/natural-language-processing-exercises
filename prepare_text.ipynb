{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11dd3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codeup: get_blog_articles(article_list)\n",
      "inshort: scrape_one_page(topic)\n",
      "inshort: get_news_articles()includes:business sports technology entertainment\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7d8571",
   "metadata": {},
   "source": [
    "# What does unicode do?\n",
    "\n",
    "\n",
    "\n",
    "print '%r' % normalize('NFD', u'\\u00C7')  # decompose: convert Ç to \"C + ̧\"\n",
    "\n",
    "print '%r' % normalize('NFC', u'C\\u0327') # compose: convert \"C + ̧\" to Ç\n",
    "\n",
    "#### circled numbers\", like ① (unicode number 2460)\n",
    "print '%r' % normalize('NFD', u'\\u2460')     # u'\\u2460'\n",
    "\n",
    "print '%r' % normalize('NFKD', u'\\u2460')    # 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e10c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "#         Lowercase everything\n",
    "#         Normalize unicode characters\n",
    "#         Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "\n",
    "def basic_clean(t):\n",
    "    article = t.lower()\n",
    "    article1 = unicodedata.normalize('NFKD', article).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    article2 = re.sub(r\"[^a-z0-9\\s']\", '', article1)\n",
    "    return article2\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea52cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = \"jfskjf;lZZZkj'sk ldj999\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb80547c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"jfskjflzzzkj'sk ldj999\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_clean(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bb04831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db05635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     2. Define a function named tokenize. It should take in a string and tokenize all the words in the string.\n",
    "\n",
    "def tokenize(t):\n",
    "    return ToktokTokenizer().tokenize(t, return_str=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99812018",
   "metadata": {},
   "outputs": [],
   "source": [
    "l= \"cat-hat'sf/ ;fs%df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df780947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"cat-hat ' sf/ ; fs % df\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fce3570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words.\n",
    "\n",
    "def stem(t):\n",
    "    l = []\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    for word in t.split():\n",
    "        stems = ps.stem(word)\n",
    "        l.append(stems)\n",
    "        art_stem = ' '.join(l)\n",
    "    return art_stem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be0ba0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem2(t):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    for word in t.split():\n",
    "        stems = ps.stem(word)\n",
    "        art_stem = ''.join(stems)\n",
    "        return art_stem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69b3c5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "469ee26c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = 'I am running and I tripped while singing to the dog in the box'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4a5b638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am run and i trip while sing to the dog in the box'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = stem(t)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea16d51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i am run and i trip while sing to the dog in the box    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(p).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a6ab031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = stem2(t)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51a9e126",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'running',\n",
       " 'and',\n",
       " 'I',\n",
       " 'tripped',\n",
       " 'while',\n",
       " 'singing',\n",
       " 'to',\n",
       " 'the',\n",
       " 'dog',\n",
       " 'in',\n",
       " 'the',\n",
       " 'box']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88bef3be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "am\n",
      "run\n",
      "and\n",
      "i\n",
      "trip\n",
      "while\n",
      "sing\n",
      "to\n",
      "the\n",
      "dog\n",
      "in\n",
      "the\n",
      "box\n"
     ]
    }
   ],
   "source": [
    "for word in t.split():\n",
    "    stems = ps.stem(word)\n",
    "    print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed3cfbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "am\n",
      "run\n",
      "and\n",
      "i\n",
      "trip\n",
      "while\n",
      "sing\n",
      "to\n",
      "the\n",
      "dog\n",
      "in\n",
      "the\n",
      "box\n"
     ]
    }
   ],
   "source": [
    "for word in t.split():\n",
    "    stems = ps.stem(word)\n",
    "    art_stem = ''.join(stems)\n",
    "    print(art_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a7be230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'running',\n",
       " 'and',\n",
       " 'I',\n",
       " 'tripped',\n",
       " 'while',\n",
       " 'singing',\n",
       " 'to',\n",
       " 'the',\n",
       " 'dog',\n",
       " 'in',\n",
       " 'the',\n",
       " 'box']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = t.split()\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a432535d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am running and I tripped while singing to the dog in the box'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = ' '.join(l)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0360ac8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30c2a9de",
   "metadata": {},
   "source": [
    "pd.Series(stems).value_counts().head(10)\n",
    "you       22\n",
    "to        21\n",
    "and       17\n",
    "a         16\n",
    "data      13\n",
    "scienc    12\n",
    "what      12\n",
    "the       10\n",
    "do        10\n",
    "we        10\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1ca0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "916661df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word.\n",
    "\n",
    "def lemmatize(t):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d108274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "#     This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove.\n",
    "\n",
    "def remove_stopwords(t):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    #stopwords_list.remove(exclude)\n",
    "    #stopwords_list.append(extra)\n",
    "\n",
    "    words = t.split()\n",
    "    filtered_words = [word for word in words if word not in stopwords_list]\n",
    "    article_without_stopwords = ' '.join(filtered_words)\n",
    "    return article_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "443f0ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 'I am myself and i, we are ourselves and we, potato potatos potatoes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1de7496",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = 'i am myself and i we are ourselves and we potato potatos potatoes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6cdd04d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add = ['potato']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67a2de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtract = ['i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d1e7f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I i, we, potato potatos potatoes'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4fc848e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'potato potatos potatoes'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45643c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: you have to clean before you can use stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "982260c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5cf9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23212c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb2db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7885dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df.\n",
    "\n",
    "news = acquire.get_news_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddf2ae14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India's GDP grows at 13.5% in first quarter of...</td>\n",
       "      <td>India's GDP grew at 13.5% in the first quarter...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 top executives at Snap quit hours after repo...</td>\n",
       "      <td>Two senior advertising executives at Snap quit...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Musk seeks to delay Twitter trial to Nov amid ...</td>\n",
       "      <td>Tesla CEO Elon Musk is seeking to delay the tr...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Snap to lay off 20% of staff, cancel several p...</td>\n",
       "      <td>Snap said on Wednesday it will lay off 20% of ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Viral video shows Amazon parcels thrown out of...</td>\n",
       "      <td>A video from Guwahati railway station has gone...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  India's GDP grows at 13.5% in first quarter of...   \n",
       "1  2 top executives at Snap quit hours after repo...   \n",
       "2  Musk seeks to delay Twitter trial to Nov amid ...   \n",
       "3  Snap to lay off 20% of staff, cancel several p...   \n",
       "4  Viral video shows Amazon parcels thrown out of...   \n",
       "\n",
       "                                             content  category  \n",
       "0  India's GDP grew at 13.5% in the first quarter...  business  \n",
       "1  Two senior advertising executives at Snap quit...  business  \n",
       "2  Tesla CEO Elon Musk is seeking to delay the tr...  business  \n",
       "3  Snap said on Wednesday it will lay off 20% of ...  business  \n",
       "4  A video from Guwahati railway station has gone...  business  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.DataFrame(news)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "036d538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df.\n",
    "\n",
    "blogs = acquire.get_blog_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0f9d058",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df = pd.DataFrame(blogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b6c766f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date_published</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is a Career in Tech Recession-Proof?</td>\n",
       "      <td>Aug 12, 2022</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nGiven the current economic climate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Codeup X Superhero Car Show &amp; Comic Con</td>\n",
       "      <td>Aug 10, 2022</td>\n",
       "      <td>\\nCodeup had a blast at the San Antonio Superh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What Jobs Can You Get After a Coding Bootcamp?...</td>\n",
       "      <td>Jul 14, 2022</td>\n",
       "      <td>\\nHave you been considering a career in Cloud ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What Jobs Can You Get After a Coding Bootcamp?...</td>\n",
       "      <td>Jul 7, 2022</td>\n",
       "      <td>\\nIf you are interested in embarking on a care...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title date_published  \\\n",
       "0               Is a Career in Tech Recession-Proof?   Aug 12, 2022   \n",
       "1            Codeup X Superhero Car Show & Comic Con   Aug 10, 2022   \n",
       "2  What Jobs Can You Get After a Coding Bootcamp?...   Jul 14, 2022   \n",
       "3  What Jobs Can You Get After a Coding Bootcamp?...    Jul 7, 2022   \n",
       "\n",
       "                                             content  \n",
       "0  \\n\\n\\n\\n\\n\\nGiven the current economic climate...  \n",
       "1  \\nCodeup had a blast at the San Antonio Superh...  \n",
       "2  \\nHave you been considering a career in Cloud ...  \n",
       "3  \\nIf you are interested in embarking on a care...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d15c55f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 For each dataframe, produce the following columns:\n",
    "\n",
    "    #title to hold the title\n",
    "    #original to hold the original article/post content\n",
    "    #clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "    #stemmed to hold the stemmed version of the cleaned data.\n",
    "    #lemmatized to hold the lemmatized version of the cleaned data.\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fe574c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ed99eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 Ask yourself: ---->                                                          Answers:\n",
    "\n",
    "#If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?    Answer: Lemmatized\n",
    "#If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?     Answer: Lemmatized\n",
    "#If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, \n",
    "#would you prefer to use stemmed or lemmatized text?                             Answer: Stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd8e474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
